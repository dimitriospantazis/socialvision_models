{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load video annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Specify the directory path\n",
    "project_dir = os.path.dirname(os.getcwd())\n",
    "annotations_file = os.path.join(project_dir,'data','annotations','video_annotations_combined_43407.csv')  # Change to your desired directory\n",
    "\n",
    "# Read the annotations file\n",
    "annotations = pd.read_csv(annotations_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate annotated video files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43407 videos\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from utils_functions import find_videos\n",
    "\n",
    "\n",
    "# Video directory\n",
    "video_dir = r'D:\\MYPROJECTS_GITHUB2\\socialvision_GPT_annotations\\videos'\n",
    "\n",
    "# Find the videos that are in the annotations file\n",
    "matched_videos = find_videos(video_dir, target_names=list(annotations['video_name']))\n",
    "print(f'Found {len(matched_videos)} videos')   # Found 43408 videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process video files to extract frames, apply transformation per frame, and save in a tensor of shape num_frames, channels, height, width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:   3%|â–Ž         | 1125/43407 [00:00<00:02, 15207.87it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of frames requested exceeds total frames in the video.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Extract frames from video\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mextract_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Apply transformations to each frame\u001b[39;00m\n\u001b[0;32m     32\u001b[0m frames \u001b[38;5;241m=\u001b[39m [transform(frame) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames]\n",
      "File \u001b[1;32md:\\MYPROJECTS_GITHUB2\\socialvision_models\\src\\utils\\utils_functions.py:54\u001b[0m, in \u001b[0;36mextract_frames\u001b[1;34m(video_path, num_frames)\u001b[0m\n\u001b[0;32m     51\u001b[0m total_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_COUNT))\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_frames \u001b[38;5;241m>\u001b[39m total_frames:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of frames requested exceeds total frames in the video.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Calculate the interval in frames to extract equispaced frames\u001b[39;00m\n\u001b[0;32m     57\u001b[0m interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_frames \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_frames)\n",
      "\u001b[1;31mValueError\u001b[0m: Number of frames requested exceeds total frames in the video."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from models import load_transform\n",
    "from utils_functions import extract_frames\n",
    "\n",
    "# Specify how many frames to extract from each video\n",
    "num_frames = 16\n",
    "\n",
    "# Load transformation pipeline\n",
    "transform = load_transform()\n",
    "\n",
    "# Create an output directory for storing result files\n",
    "output_dir = os.path.join(project_dir,'data','processed_videos')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each video in `matched_videos`\n",
    "for video in tqdm(matched_videos, desc=\"Processing videos\"):\n",
    "    # Construct an output filename based on the video name\n",
    "    video_name = os.path.basename(video)\n",
    "    base_name, _ = os.path.splitext(video_name)\n",
    "    output_path = os.path.join(output_dir, f\"{base_name}.pt\")\n",
    "\n",
    "    # Skip processing if the output file already exists\n",
    "    if os.path.exists(output_path):\n",
    "        # print(f\"Skipping {video_name} (already processed).\")\n",
    "        continue\n",
    "\n",
    "    # Extract frames from video\n",
    "    try:\n",
    "        frames = extract_frames(video, num_frames=num_frames)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_name}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Apply transformations to each frame\n",
    "    frames = [transform(frame) for frame in frames]\n",
    "\n",
    "    # Stack frames to create a tensor of shape [num_frames, channels, height, width]\n",
    "    video_tensor = torch.stack(frames, dim=1)\n",
    "\n",
    "    # Save the tensor to a .pt file\n",
    "    torch.save(video_tensor, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A child process terminated abruptly, the process pool is not usable anymore",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Process all videos in parallel, skipping those that are already processed\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[43mprocess_videos_in_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatched_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 63\u001b[0m, in \u001b[0;36mprocess_videos_in_parallel\u001b[1;34m(matched_videos, num_frames, output_dir)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Use a ProcessPoolExecutor to run tasks in parallel\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Pass the 'worker' function as the first argument, and our jobs list as the second\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     results_iter \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Wrap results in a tqdm progress bar\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m tqdm(results_iter, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(jobs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing videos\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;66;03m# Optionally, print each result to see \"Processed\" or \"Skipping\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# print(result)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\process.py:859\u001b[0m, in \u001b[0;36mProcessPoolExecutor.map\u001b[1;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunksize must be >= 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 859\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_process_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m_get_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chain_from_iterable_of_lists(results)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:608\u001b[0m, in \u001b[0;36mExecutor.map\u001b[1;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    606\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 608\u001b[0m fs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39miterables)]\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;66;03m# before the first iterator value is required.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult_iterator\u001b[39m():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\process.py:813\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_lock:\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken:\n\u001b[1;32m--> 813\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BrokenProcessPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken)\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_thread:\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot schedule new futures after shutdown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A child process terminated abruptly, the process pool is not usable anymore"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from models import load_transform\n",
    "from utils_functions import extract_frames\n",
    "\n",
    "def process_single_video(video_path, num_frames, output_dir):\n",
    "    \"\"\"\n",
    "    Extract frames, apply transforms, and save the resulting tensor to .pt file.\n",
    "    Skips processing if the .pt file already exists.\n",
    "    \"\"\"\n",
    "    # Create the output file path based on the video filename\n",
    "    video_name = os.path.basename(video_path)\n",
    "    base_name, _ = os.path.splitext(video_name)\n",
    "    output_path = os.path.join(output_dir, f\"{base_name}.pt\")\n",
    "\n",
    "    # Skip if this file has already been processed\n",
    "    if os.path.exists(output_path):\n",
    "        return f\"Skipping (file exists): {video_path}\"\n",
    "\n",
    "    # Load transforms (must be picklable if using ProcessPoolExecutor)\n",
    "    transform = load_transform()\n",
    "\n",
    "    # Extract frames from the video\n",
    "    frames = extract_frames(video_path, num_frames=num_frames)\n",
    "\n",
    "    # Apply transformations to each frame\n",
    "    frames = [transform(frame) for frame in frames]\n",
    "\n",
    "    # Stack frames to create a tensor of shape [num_frames, channels, height, width]\n",
    "    video_tensor = torch.stack(frames, dim=1)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the tensor to a .pt file\n",
    "    torch.save(video_tensor, output_path)\n",
    "    return f\"Processed: {video_path}\"\n",
    "\n",
    "def worker(args):\n",
    "    \"\"\"\n",
    "    A top-level worker function to unpack the arguments and call the real processing function.\n",
    "    This must be defined at the module level (not nested) so it can be pickled.\n",
    "    \"\"\"\n",
    "    video_path, num_frames, output_dir = args\n",
    "    return process_single_video(video_path, num_frames, output_dir)\n",
    "\n",
    "def process_videos_in_parallel(matched_videos, num_frames=16, output_dir=\"processed_videos\"):\n",
    "    \"\"\"\n",
    "    Use ProcessPoolExecutor to process each video in matched_videos in parallel.\n",
    "    Skips any videos that already have a .pt file in the output_dir.\n",
    "    \"\"\"\n",
    "    # Create output directory if needed\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare a list of (args) for each video\n",
    "    jobs = [(video_path, num_frames, output_dir) for video_path in matched_videos]\n",
    "\n",
    "    # Use a ProcessPoolExecutor to run tasks in parallel\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        # Pass the 'worker' function as the first argument, and our jobs list as the second\n",
    "        results_iter = executor.map(worker, jobs)\n",
    "\n",
    "        # Wrap results in a tqdm progress bar\n",
    "        for result in tqdm(results_iter, total=len(jobs), desc=\"Processing videos\"):\n",
    "            # Optionally, print each result to see \"Processed\" or \"Skipping\"\n",
    "            # print(result)\n",
    "            pass\n",
    "\n",
    "# Create an output directory for storing result files\n",
    "output_dir = os.path.join(project_dir,'data','processed_videos')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process all videos in parallel, skipping those that are already processed\n",
    "process_videos_in_parallel(\n",
    "    matched_videos,\n",
    "    num_frames=16,\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from models import load_transform\n",
    "from utils_functions import extract_frames\n",
    "\n",
    "# Specify how many frames to extract from each video\n",
    "num_frames = 16\n",
    "\n",
    "# Load transformation pipeline\n",
    "transform = load_transform()\n",
    "\n",
    "# Create an output directory for storing result files\n",
    "output_dir = os.path.join(project_dir, 'data', 'processed_videos')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def process_video(video):\n",
    "    \"\"\"\n",
    "    Extract frames from a single video, transform them,\n",
    "    create a tensor, and save it to disk (unless it already exists).\n",
    "    \"\"\"\n",
    "    # Construct an output filename based on the video name\n",
    "    video_name = os.path.basename(video)\n",
    "    base_name, _ = os.path.splitext(video_name)\n",
    "    output_path = os.path.join(output_dir, f\"{base_name}.pt\")\n",
    "\n",
    "    # Skip if the file is already there\n",
    "    if os.path.isfile(output_path):\n",
    "        print(f\"Skipping {video_name}. Output already exists: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    # Extract frames from video\n",
    "    frames = extract_frames(video, num_frames=num_frames)\n",
    "\n",
    "    # Stack frames to create a tensor of shape [num_frames, channels, height, width]\n",
    "    video_tensor = torch.stack(frames, dim=1)\n",
    "\n",
    "    # Save the tensor to a .pt file\n",
    "    torch.save(video_tensor, output_path)\n",
    "\n",
    "    return output_path  # Just to have a return value if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43407 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos in parallel:   0%|          | 0/43407 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(matched_videos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m videos\u001b[39m\u001b[38;5;124m'\u001b[39m)   \u001b[38;5;66;03m# Found 43408 videos\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatched_videos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmatched_videos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessing videos in parallel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MYPROJECTS_GITHUB2\\socialvision_models\\MiTmodels_venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\process.py:642\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[0;32m    637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "# Cell 2: The main guard\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # Video directory\n",
    "    video_dir = r'D:\\MYPROJECTS_GITHUB2\\socialvision_GPT_annotations\\videos'\n",
    "\n",
    "    # Find the videos that are in the annotations file\n",
    "    matched_videos = find_videos(video_dir, target_names=list(annotations['video_name']))\n",
    "    print(f'Found {len(matched_videos)} videos')   # Found 43408 videos\n",
    "\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "        list(tqdm(\n",
    "            executor.map(process_video, matched_videos),\n",
    "            total=len(matched_videos),\n",
    "            desc=\"Processing videos in parallel\"\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiTmodels_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
